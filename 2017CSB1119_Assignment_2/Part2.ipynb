{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Vineet Madan\n",
    "Purpose: Lab Assignment 2 Part 2 of CS529 Applied Artificial Intelligence\n",
    "Date: 8 October 2019\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import json\n",
    "import random\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import csv\n",
    "from nltk.stem import PorterStemmer \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = open('nytimes_news_articles.txt', 'r')\n",
    "DATA_LINES = DATA_FILE.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URLS = []\n",
    "DATA_TEXT = []\n",
    "\n",
    "for line in DATA_LINES:\n",
    "    if line.startswith('URL'):\n",
    "        DATA_URLS.append(line)\n",
    "        DATA_TEXT.append([])\n",
    "    else:\n",
    "        DATA_TEXT[-1].append(line)\n",
    "        \n",
    "#clean the text a bit\n",
    "for txt in DATA_TEXT:\n",
    "    try:\n",
    "        while True:\n",
    "            txt.remove('\\n')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        while True:\n",
    "            txt.remove('')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        while True:\n",
    "            txt.remove(' ')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        while True:\n",
    "            txt.remove('\\t')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#     print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URLS = [url[27: len(url)-6] for url in DATA_URLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. starts here\n",
    "\"\"\"\n",
    "\n",
    "tknzd_urls = []\n",
    "categories = []\n",
    "subcategories = []\n",
    "for url in DATA_URLS:\n",
    "    temp = url.split('/')\n",
    "    if temp[-3].isnumeric()==True:\n",
    "        category = temp[-2]\n",
    "        headline = temp[-1]\n",
    "        headline= headline.replace('-', ' ')\n",
    "        subcategory = ''\n",
    "        if category not in categories:#add only if unique\n",
    "            categories.append(category)\n",
    "    else:\n",
    "        category = temp[-3]\n",
    "        subcategory = temp[-2]\n",
    "        headline = temp[-1]\n",
    "        headline= headline.replace('-', ' ')\n",
    "        if category not in categories:#add only if unique\n",
    "            categories.append(category)\n",
    "        if subcategory not in subcategories:#add only if unique\n",
    "            subcategories.append(subcategory)\n",
    "        \n",
    "    tknzd_urls.append([category, subcategory, headline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzd_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nytimes_processed_dset.csv', mode='w') as csv_file:\n",
    "    url_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for tkn_url, content in zip(tknzd_urls, DATA_TEXT):\n",
    "            url_writer.writerow(tkn_url+[content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique  categories = {}; Number of unique subcategories = {};\".format(len(categories), len(subcategories)))\n",
    "\"\"\"\n",
    "Q1 ends here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEXT = DATA_TEXT[:500]\n",
    "len(DATA_TEXT)\n",
    "NUM_DOCS=500#reduce the size of the dataset to comply with hardware realities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#construct the word corpus\n",
    "###\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "CORPUS = []\n",
    "WORDS_IN_DOCS = []\n",
    "\n",
    "for lines in DATA_TEXT:\n",
    "    WORDS_IN_DOCS.append([])\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            try:\n",
    "                while True:\n",
    "                    word.remove('\"')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                while True:\n",
    "                    word.remove(' ')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                while True:\n",
    "                    word.remove('\\'')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            word = ''.join(e for e in word if e.isalpha())\n",
    "            if word not in stop_words and word!='':\n",
    "                CORPUS.append(word)\n",
    "                WORDS_IN_DOCS[-1].append(word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(CORPUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### do stemming\n",
    "\n",
    "#dict whose key is stem and value is the corresponding word\n",
    "stems = {}\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for w_id in range(len(CORPUS)): \n",
    "    if w_id%2000000==0:\n",
    "        print(w_id)\n",
    "    stem = ps.stem(CORPUS[w_id])\n",
    "    if stem not in stems.keys():\n",
    "        stems[stem] = [CORPUS[w_id].lower(), 1]\n",
    "    else:\n",
    "        stems[stem][1] = stems[stem][1]+1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = len(stems)\n",
    "# NUM_DOCS = len(DATA_URLS)\n",
    "\n",
    "WORD_VEC_MAT = np.zeros(shape=(NUM_DOCS, NUM_WORDS), dtype='uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DOCS, NUM_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_id_from_word(stems, word):\n",
    "    i = 0\n",
    "    for key in stems.keys():\n",
    "        if key.lower()==word.lower():\n",
    "            return i\n",
    "        i=i+1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the word vector matrix by iterating through the WORDS_IN_DOCS\n",
    "load_word_vec_mat_from_file = True\n",
    "\n",
    "if load_word_vec_mat_from_file==True:\n",
    "    WORD_VEC_MAT=np.load('word_vec_model_red_red.npy')\n",
    "else:\n",
    "    WORD_VEC_MAT.fill(0)\n",
    "    for doc_id in range(NUM_DOCS):\n",
    "        print(doc_id)\n",
    "        for w_index in range(len(WORDS_IN_DOCS[doc_id])):\n",
    "            st = ps.stem(WORDS_IN_DOCS[doc_id][w_index])\n",
    "            w_id = get_w_id_from_word(stems, st)\n",
    "            WORD_VEC_MAT[doc_id][w_id] = WORD_VEC_MAT[doc_id][w_id] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the WORD_VEC_MAT to file...its a np array\n",
    "if not load_word_vec_mat_from_file:\n",
    "    np.save('word_vec_model_red_red.npy', WORD_VEC_MAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_VEC_MAT = WORD_VEC_MAT.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get the IDF for each of the words...as a numpy array of size [NUM_WORDS] and save it to file\n",
    "#find the idf for each word\n",
    "WORD_DOC_FREQS = np.zeros(NUM_WORDS)\n",
    "\n",
    "load_word_doc_freqs_from_file = True\n",
    "\n",
    "if load_word_doc_freqs_from_file==True:\n",
    "    WORD_DOC_FREQS = np.load('word_doc_freqs_red_red.npy')\n",
    "else:\n",
    "    for w_id in range(NUM_WORDS):\n",
    "        if w_id%10==0:\n",
    "            print(w_id)\n",
    "        freq = 0\n",
    "        for doc_id in range(NUM_DOCS):\n",
    "            if WORD_VEC_MAT[doc_id][w_id] > 0:\n",
    "                freq+=1\n",
    "        WORD_DOC_FREQS[w_id] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_word_doc_freqs_from_file:\n",
    "    np.save('word_doc_freqs_red_red.npy', WORD_DOC_FREQS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INV_DOC_FREQS = np.zeros(NUM_WORDS)\n",
    "INV_DOC_FREQS.fill(NUM_DOCS)\n",
    "for ith in range(INV_DOC_FREQS.shape[0]):\n",
    "    if not WORD_DOC_FREQS[ith]==0:\n",
    "        INV_DOC_FREQS[ith] = np.log(NUM_DOCS/WORD_DOC_FREQS[ith])\n",
    "\n",
    "\"\"\"\n",
    "The final tfidf marix is defined here\n",
    "\"\"\"\n",
    "FINAL_TFIDF_MATRIX = np.copy(WORD_VEC_MAT)\n",
    "for doc_id in range(NUM_DOCS):\n",
    "    FINAL_TFIDF_MATRIX[doc_id] = np.multiply(WORD_VEC_MAT[doc_id], INV_DOC_FREQS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3. LSA\n",
    "\"\"\"\n",
    "#convert a 1d matrix of singluar values returned by the svd function to a 2d square matrix\n",
    "def convert_singluar_to_diagonal(mat):\n",
    "    sz = mat.shape[0]\n",
    "    res = np.zeros(shape=(sz, sz))\n",
    "    for i in range(sz):\n",
    "        res[i][i] = mat[i]\n",
    "    return res\n",
    "\n",
    "#the input is the word_to_vector matrix after applying tfidf\n",
    "def do_lsa(final_tfidf_mat, k=100):\n",
    "    num_docs = final_tfidf_mat.shape[0]\n",
    "    num_terms = final_tfidf_mat.shape[1]\n",
    "    \n",
    "    u,s,v = np.linalg.svd(final_tfidf_mat)\n",
    "    \n",
    "    s = convert_singluar_to_diagonal(s[:k])\n",
    "    \n",
    "    u,s,v = np.linalg.svd(final_tfidf_mat)\n",
    "    s = np.reshape(s, (s.shape[0], 1))\n",
    "\n",
    "    s = convert_singluar_to_diagonal(s)\n",
    "\n",
    "    #get the top-k eigenvals etc.\n",
    "    u = u[:, :k]\n",
    "    v = v[:, :k]\n",
    "    s = s[:k, :k]\n",
    "    \n",
    "    return u,s,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,v = do_lsa(final_tfidf_mat=FINAL_TFIDF_MATRIX, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4. PLSA begins here\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
